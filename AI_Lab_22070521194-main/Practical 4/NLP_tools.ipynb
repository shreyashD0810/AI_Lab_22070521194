{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. It enables machines to understand, interpret, and generate text in a way that is both meaningful and contextually appropriate. NLP techniques are widely used in applications such as chatbots, speech recognition, sentiment analysis, and machine translation. The process involves various steps, including tokenization, stopword removal, stemming, and lemmatization, which help break down and analyze text more effectively. Advances in deep learning and neural networks have significantly improved NLP models, making them more accurate and capable of handling complex linguistic structures. From customer support automation to content recommendation systems, NLP plays a crucial role in enhancing human-computer interaction and streamlining various business processes. As technology evolves, the potential of NLP continues to expand, driving innovation in fields like healthcare, finance, and education.\"\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Tokenization\n",
        "words = word_tokenize(sample_text)\n",
        "sentences = sent_tokenize(sample_text)\n",
        "\n",
        "# Stopword Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "# Output Results\n",
        "print(\"Original Text:\", sample_text)\n",
        "print(\"\\nTokenized Words:\", words)\n",
        "print(\"\\nTokenized Sentences:\", sentences)\n",
        "print(\"\\nFiltered Words (Stopword Removal):\", filtered_words)\n",
        "print(\"\\nStemmed Words:\", stemmed_words)\n",
        "print(\"\\nLemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "id": "jKhLAL1Dbju8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}