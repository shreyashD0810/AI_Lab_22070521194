{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d96df43-8c13-4aeb-8558-8da36b711725",
   "metadata": {},
   "source": [
    "#### Perform text preprocessing on the chosen case study application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6490389b-caa2-4361-beb9-2a8af0805996",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd4fdf1d-ff47-4461-bd7d-46a7d92986d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'can', 'do', 'so', 'much', 'with', 'natural', 'language', 'processing', 'ranging', 'from', 'speech', 'recognition', ',', 'recommendation', 'systems', ',', 'spam', 'classifiers', ',', 'and', 'so', 'many', 'more', 'applications', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text_example = \"\"\"We can do so much with Natural Language Processing ranging \n",
    "from speech recognition, recommendation systems, spam classifiers, and so \n",
    "many more applications.\"\"\"\n",
    "\n",
    "word_tokenized_sent = word_tokenize(text_example.casefold())\n",
    "print(word_tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b311589-9353-4641-8257-57e49d0fc7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we can do so much with natural language processing ranging from\\nspeech recognition, recommendation systems, spam classifiers, and so many more applications.', 'these applications also leverage the power of machine learning and deep learning.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text_example2 = \"\"\"We can do so much with Natural Language Processing ranging from\n",
    "speech recognition, recommendation systems, spam classifiers, and so many more applications.\n",
    "These applications also leverage the power of Machine Learning and Deep Learning.\n",
    "\"\"\"\n",
    "\n",
    "sentence_tokenized= sent_tokenize(text_example2.casefold())\n",
    "print(sentence_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdd2d52-2da9-4640-95b7-cf04c9ad9677",
   "metadata": {},
   "source": [
    "### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f604f2c4-02a8-4966-b3fc-11642cbc161e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['much', 'natural', 'language', 'processing', 'ranging', 'speech', 'recognition', ',', 'recommendation', 'systems', ',', 'spam', 'classifiers', ',', 'many', 'applications', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kgadg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "eng_stopwords= stopwords.words('english')\n",
    "\n",
    "# filter stopwords with list comprehension\n",
    "tokens_no_stopwords = [word for word in word_tokenized_sent if word not in eng_stopwords]\n",
    "print(tokens_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf5288e-70bd-4cc9-873b-40120e127262",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac4f3abb-22fd-4396-b20a-180299f35076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens :['much', 'natur', 'languag', 'process', 'rang', 'speech', 'recognit', ',', 'recommend', 'system', ',', 'spam', 'classifi', ',', 'mani', 'applic', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens_no_stopwords]\n",
    "print(f\"Stemmed Tokens :{stemmed_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50ddefd-b712-43da-8f43-c52f9eda45fa",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "526fa0be-cab9-4ca4-8740-a3f2f0177da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['much', 'natural', 'language', 'processing', 'ranging', 'speech', 'recognition', ',', 'recommendation', 'system', ',', 'spam', 'classifier', ',', 'many', 'application', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens_no_stopwords]\n",
    "print(f\"Lemmatized Tokens: {lemmatized_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c6571-559b-4502-9072-53715e05b7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
